---
title: "607 Assignment 9"
author: "Ariann Chai"
date: "2023-11-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This assignment was to read in JSON data from one of NY Times APIs and turn into a dataframe. I decide to use the Article Search API to look up articles about Queen Elizabeth from the day of her death to the end of that year.  

The first step is to call the necessary libraries.  

```{r libraries}
library(dplyr)
library(httr)
library(jsonlite)
library(kableExtra)
library(stringr)
```

## Get articles  

In order to get these articles, I needed to sign up for an API key which I then saved as a variable. I also saved my search term/word (Queen), and the two dates I would like to find articles in between. Combined, this creates the search url that will be used.    

```{r variables}
apiKey <- "hSndMn20mNAgN0TcTP20NOY4Xc01dtuP"
term <- "Queen"
subsection <- "Europe"
begin_date <- "20220908"
end_date <- "20221231"

searchUrl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&fq=subsection_name:",subsection,
                  "&begin_date=",begin_date,
                  "&end_date=",end_date,
                  "&facet_filter=true&api-key=",apiKey, 
                  sep="")
```

Next was to execute the search url that results in an query and this gives us a raw/unorganized query of articles. Using a for loop, we separate the articles into their own rows in dataframe, articles. Articles still prints out messy as there are a lot of columns and information that the search query gives that is unnecessary.  
```{r query}
searchQuery <- fromJSON(searchUrl)
maxPages <- round((searchQuery$response$meta$hits[1] / 10)-1) 
pages <- vector("list",length=maxPages)
```

```{r search, error=TRUE, echo=TRUE, message=FALSE, warning=FALSE, out.width="100%"}
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(searchUrl, "&page=", i), flatten = TRUE) %>% data.frame()
  pages[[i+1]] <- nytSearch
  Sys.sleep(5)
}
articles <- rbind_pages(pages)
head(articles, 1)
```

## Cleaning/Tidying the dataframe

After getting articles as a raw dataframe, I cleaned up the column names a bit. I then determined which columns I would want to keep as some had irrelevant data or no data (NA). The selected columns were then subsetted into a new dataframe, queenArticles, that acts as our final dataframe after doing some column renaming.  

```{r columns, echo=TRUE, message=FALSE, warning=FALSE, out.width="100%"}
colnames(articles)
colnames(articles) <- str_replace(colnames(articles),pattern='response.docs\\.',replace='')
colnames(articles) <- str_replace(colnames(articles),pattern='response\\.',replace='')
colnames(articles)

queenArticles<-as.data.frame(articles)

#columns to keep: copyright,web_url,pub_date,type_of_material,word_count,headline.main,byline.original
queenArticles <- subset(queenArticles, select=c(2,4,12,16,18,21,28))

colnames(queenArticles)[3]="publish_date"
colnames(queenArticles)[4]="type_of_article"
colnames(queenArticles)[6]="headline"
colnames(queenArticles)[7]="original_author"

kable(queenArticles, "html") %>% kable_styling("striped") %>% scroll_box(width = "100%")

```

With this, the data/articles collected using NY Times Article Search API is cleaned and ready for analysis. 